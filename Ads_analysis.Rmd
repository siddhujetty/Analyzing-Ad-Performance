---
title: "Ads Analysis"
author: "Siddhartha Jetti"
date: "July 17, 2019"
output: rmarkdown::github_document
---

# Goal

Maybe the first industry to heavily rely on data science was the online ads industry. Data Science is used to choose which ads to show, how much to pay, optimize the ad text and the position as well as in countless of other related applications.

Optimizing ads is one of the most intellectually challenging jobs a data scientist can do. It is a really complex problem given the huge (really really huge) size of the datasets as well as number of features that can be used. 
Moreover, companies often spend huge amounts of money in ads and a small ad optimization improvement can be worth millions of dollars for the company.

The goal of this project is to look at a few ad campaigns and analyze their current performance as well as predict their future performance.


# Challenge Description

Company XYZ is a food delivery company. Like pretty much any other site, in order to get customers, they have been relying significantly on online ads, such as those you see on Google or Facebook.

At the moment, they are running 40 different ad campaigns and want you to help them understand their performance.

Specifically, you are asked to:

1) If you had to identify the 5 best ad groups, which ones would be? Which metric did you choose to identify the best ones? Why? Explain the pros of your metric as well as the possible cons. From a business perspective, choosing that metric implies that you are focusing on what?


2) For each group, predict how many ads will be shown on Dec, 15 (assume each ad group keeps following its trend).


3) Cluster ads into 3 groups: the ones whose avg_cost_per_click is going up, the ones whose avg_cost_per_click is flat and the ones whose avg_cost_per_click is going down.


## Data

We have 1 table downloadable by clicking here.

The table is:

ad_table - aggregate information about ads

### Columns:

* date : all data are aggregated by date
* shown : the number of ads shown on a given day all over the web. Impressions are free. That is, companies pay only if a user clicks on the ad, not to show it
* clicked : the number of clicks on the ads. This is what companies pay for. By clicking on the ad, the user is brought to the site
converted : the number of conversions on the site coming from ads. To be counted, a conversion as to happen on the same day as the ad click.
* avg_cost_per_click : on an average, how much it cost each of those clicks
* total_revenue : how much revenue came from the conversions
* ad : we have several different ad versions with different text. This shows which ad group we are considering


# Problem Setup

```{r}
# Load required libraries
library(dplyr)
library(ggplot2)
library(lubridate)
library(randomForest)

# Read in the input data into a dataframe
ads <- read.csv("ad_table.csv", stringsAsFactors = F) 
```

# Data Exploration

Explore the ads dataset
```{r}
# Transform variables in ads dataset
ads <- ads %>%
  mutate(date = as.Date(date)) %>%
  arrange(ad, date)

# Check data types of each of the columns
summary(ads)

# Take a peek at data
head(ads)
```

Removing rows with shown = 0 or revenue < 0

```{r}
ads_cleaned <- ads %>%
  filter(shown > 0, clicked > 0, total_revenue >= 0)
```

Now, checking the summary of dataset

```{r}
summary(ads_cleaned)
```

Check for missing values in the data.
```{r}
# count of missing values by column in views dataset
colSums(is.na(ads_cleaned))
```
No missing values exist anywhere in the data.

Check if data exists for all the ads for all the dates.
```{r}
table(ads_cleaned$date)
```

```{r}
table(ads_cleaned$ad)
```

Overall the data looks good.

# Question 1:

Here the goal is to choose the best 5 ads based on the provided data.
I plan on using average return on advertising budget as a metric to choose the top ad campaigns.

```{r}
# calculate average daily return for every ad
best_ads_avg_revenue <- ads_cleaned %>%
  group_by(ad) %>%
  summarise(avg_return =  sum(total_revenue) / sum(clicked * avg_cost_per_click)) %>%
  arrange(desc(avg_return)) 

head(best_ads_avg_revenue, 5)
```

The problem with using average return as a metric is, it does not identify the ads with low average return but are consistently trending up. Arguably, the ads that are trending up are equally important to marketing teams, if not more, than the ones with high average return and trending down. So, it makes sense to look both the average return and the trend to know the complete story.

```{r}
# Function to extract trend in the data
slope_trendline <- function(y, x) {
  trendline <- lm(formula = y ~ x)
  return(trendline$coefficients[2])
}

# Function to compute the p-valuye of x coefficient
slope_p_value <- function(y, x) {
  trendline <- lm(formula = y ~ x)
  df <- data.frame(summary(trendline)$coefficients)
  return(df[2,4])
}

# Estimate the trend over time for every ad
best_ads <- ads_cleaned %>%
  group_by(ad) %>%
  mutate(date = as.numeric(date),
         day_return =  total_revenue / (clicked * avg_cost_per_click)) %>%
  summarise(trend = slope_trendline(day_return, date)) %>%
  inner_join(best_ads_avg_revenue, by = "ad") %>%
  arrange(desc(avg_return))

head(best_ads, 10)
```

All the top 5 ad campaigns by average return are making money (return > 1).
Among them, The ad groups 31, 16 and 14 are having a positive trend. While groups 2 and 16 have high average return but are trending down.

# Question 2:

Here the goal is to predict how many times the ad will be shown on a future date. 

The simplest method is to fit a straight line that is closest to the data points for each ad group and use it to predict future views. 

Let's pick few arbitrary videos and visualize the time series of views.
```{r}
ads_cleaned %>%
  filter(ad == "ad_group_31") %>%

ggplot(aes(date, shown)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
ads_cleaned %>%
  filter(ad == "ad_group_38") %>%

ggplot(aes(date, shown)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Fitting a line to time series of all the ad campaigns

```{r}

unique_ads <- unique(ads_cleaned$ad)
new <- data.frame(x = as.numeric(as.Date("2015-12-15")))
prediction <- data.frame(ad = c(), prediction = c())

for(i in unique_ads) {
  x <- as.numeric(ads_cleaned$date[ads_cleaned$ad == i])
  y <- ads_cleaned$shown[ads_cleaned$ad == i]
  predicted <- round(predict(lm(y ~ x), newdata = new), digits = 0)
  predicted_df <- data.frame(ad = i, prediction = predicted, stringsAsFactors = F)
  prediction <- bind_rows(prediction, predicted_df)
}

head(prediction)
```

# Question 3:

First fit a linear regression line to extract the trend from data.

```{r}
# Function to normalize the variables
normalize <- function(x) {
    return((x - min(x)) / (max(x) - min(x)))
}

# Now group by video id, normalize variables and extract slope
ads_cpc_summary <- ads_cleaned %>%
  mutate(date = as.numeric(date),
         cpc_norm = normalize(avg_cost_per_click),
         date_norm = normalize(date)) %>%
  group_by(ad) %>%
  summarise(cpc_slope = slope_trendline(cpc_norm, date_norm),
            cpc_slope_p_value = slope_p_value(cpc_norm, date_norm))

# Take a look at the data
head(ads_cpc_summary)
```


The distribution of slopes of cost per click trendlines
```{r}
quantile(ads_cpc_summary$cpc_slope, probs = seq(0, 1, by = 0.05)) 
ads_cpc_summary %>%
  ggplot() +
  geom_histogram(bins = 30, aes(x = cpc_slope)) 
```

Trends are extracted by fitting a inear regression line to time series of average cost per click data. Here are rules that can be thought of to classify ads based on if average cost per click are going up, staying flat or going down.

* If coefficient of x-term is statistically significant and coefficient > 0, then average cost per clicks are going up.
* If coefficient is statistically significant and coefficient < 0, then average cost per clicks are going down.
* If coefficient is NOT statistically significant then average cost per clicks are flat.

The criterion to decide statistical significance should not be just p < 0.05. The reason is we are effectively doing 40 different tests. So, to prevent risk of incorrectly rejecting a null hypothesis due to multiple comparisons, the p-values of each test need to be adjusted using Boniferoni correction (pvalue/number of comparisons).

```{r}
# Classifying ads based on the stated rules
ads_cpc_summary <- ads_cpc_summary %>%
  mutate(cpc_category = case_when(
                      cpc_slope_p_value < (0.05/40) & cpc_slope > 0 ~ "Going up",
                      cpc_slope_p_value < (0.05/40) & cpc_slope < 0 ~ "Going down",
                      TRUE ~ "Flat")
  )

# Frequency of different video categories
table(ads_cpc_summary$cpc_category)
```

Employing Boniferoni correction is classifying all the campaigns as average cost per click remaining Flat. The reason being Boniferoni correction generally  imposes a very stringent condition for significance. 
